{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChVenkatSai/Scratch-GPT/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtyping"
      ],
      "metadata": {
        "id": "dxMhTqs9hJlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iEUGLw7qcBkC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "from torchtyping import TensorType\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from torch.utils.data import Subset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(0)\n",
        "        self.context_length = context_length\n",
        "        self.model_dim = model_dim\n",
        "        self.word = nn.Embedding(vocab_size,model_dim)\n",
        "        self.position = nn.Embedding(context_length,model_dim)\n",
        "        od = OrderedDict()\n",
        "        for i in range(num_blocks):\n",
        "            od.update({f\"block {i}\":self.TransformerBlock(model_dim,num_heads)})\n",
        "        self.layer = nn.Sequential(od)\n",
        "        self.norm = nn.LayerNorm(model_dim)\n",
        "        self.final = nn.Linear(model_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context: TensorType[int]) -> TensorType[float]:\n",
        "        torch.manual_seed(0)\n",
        "        word_embed = self.word(context)\n",
        "        batch_size = context.shape[0]\n",
        "        pos = torch.arange(self.context_length).to(device)\n",
        "        pos_embed = self.position(pos).unsqueeze(0).expand(batch_size,-1,-1)\n",
        "        embed = word_embed+pos_embed\n",
        "        block = self.layer(embed)\n",
        "        norm = self.norm(block)\n",
        "        fin = self.final(norm)\n",
        "        return fin\n",
        "\n",
        "    class TransformerBlock(nn.Module):\n",
        "\n",
        "      def __init__(self, model_dim: int, num_heads: int):\n",
        "          super().__init__()\n",
        "          torch.manual_seed(0)\n",
        "          self.first = nn.LayerNorm(model_dim)\n",
        "          self.attention = self.MultiHeadedSelfAttention(model_dim,model_dim,num_heads)\n",
        "          self.second = nn.LayerNorm(model_dim)\n",
        "          self.linear = self.VanillaNeuralNetwork(model_dim)\n",
        "\n",
        "      def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "          torch.manual_seed(0)\n",
        "          norm1 = self.first(embedded)\n",
        "          first = self.attention(norm1)\n",
        "          new = first+embedded\n",
        "          norm2 = self.second(new)\n",
        "          second = self.linear(norm2)\n",
        "          return second + new\n",
        "\n",
        "      class MultiHeadedSelfAttention(nn.Module):\n",
        "\n",
        "        def __init__(self, embedding_dim: int, attention_dim: int, num_heads: int):\n",
        "            super().__init__()\n",
        "            torch.manual_seed(0)\n",
        "            self.head_size = attention_dim//num_heads\n",
        "            self.multiattention = nn.ModuleList([self.SingleHeadAttention(embedding_dim,self.head_size) for _ in range(num_heads)])\n",
        "\n",
        "\n",
        "        def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "            l = []\n",
        "            for layer in (self.multiattention):\n",
        "                l.append(layer(embedded))\n",
        "            return torch.cat(l, dim=2)\n",
        "\n",
        "        class SingleHeadAttention(nn.Module):\n",
        "\n",
        "          def __init__(self, embedding_dim: int, attention_dim: int):\n",
        "              super().__init__()\n",
        "              torch.manual_seed(0)\n",
        "              self.attention_dim = attention_dim\n",
        "              self.key = nn.Linear(embedding_dim,attention_dim,bias=False)\n",
        "              self.query = nn.Linear(embedding_dim,attention_dim,bias=False)\n",
        "              self.value = nn.Linear(embedding_dim,attention_dim,bias=False)\n",
        "\n",
        "          def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "              keys = self.key(embedded)\n",
        "              queries = self.query(embedded)\n",
        "              values = self.value(embedded)\n",
        "              attention = torch.matmul(queries,torch.transpose(keys,1,2))\n",
        "              attention = attention/torch.sqrt(torch.tensor(self.attention_dim))\n",
        "              mask = torch.ones(attention.shape)\n",
        "              mask = torch.triu(mask,diagonal=1).bool()\n",
        "              masked = attention.masked_fill(mask.to(device), float('-inf'))\n",
        "              norm = nn.functional.softmax(masked,dim=2)\n",
        "              final = torch.matmul(norm,values)\n",
        "              return final\n",
        "\n",
        "      class VanillaNeuralNetwork(nn.Module):\n",
        "\n",
        "        def __init__(self, model_dim: int):\n",
        "            super().__init__()\n",
        "            torch.manual_seed(0)\n",
        "            self.up_projection = nn.Linear(model_dim, model_dim * 4)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.down_projection = nn.Linear(model_dim * 4, model_dim)\n",
        "            self.dropout = nn.Dropout(0.2) # using p = 0.2\n",
        "\n",
        "        def forward(self, x: TensorType[float]) -> TensorType[float]:\n",
        "            torch.manual_seed(0)\n",
        "            return self.dropout(self.down_projection(self.relu(self.up_projection(x))))"
      ],
      "metadata": {
        "id": "zHAI6-2icJ00"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Download a book from Project Gutenberg\n",
        "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"  # Example: Pride and Prejudice\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower()\n",
        "text = text.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
        "text = ''.join([c for c in text if c.isalpha() or c.isspace()])\n",
        "\n",
        "# Prepare the dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_length):\n",
        "        self.text = text\n",
        "        self.context_length = context_length\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.vocab)}\n",
        "        self.data = self.encode(text)\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data[idx:idx + self.context_length]\n",
        "        target = self.data[idx + 1:idx + self.context_length + 1]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(set(text))\n",
        "context_length = 128\n",
        "model_dim = 512\n",
        "num_blocks = 6\n",
        "num_heads = 8\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Instantiate the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Load dataset\n",
        "dataset = TextDataset(text, context_length)\n",
        "subset_indices = random.sample(range(len(dataset)), k=100000)  # Choose samples randomly if training size is large for free compute model i.e T4 GPU\n",
        "print(\"Unencoded text:\", dataset.text[:context_length + 1])\n",
        "print(dataset[0])\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (context, target) in enumerate(dataloader):\n",
        "        context, target = context.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(context)\n",
        "\n",
        "        # Reshape output and target to match the dimensions for CrossEntropyLoss\n",
        "        output = output.view(-1, vocab_size)\n",
        "        target = target.view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 500 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "nYpNdzyanRAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training is complete\n",
        "WEIGHT_PATH = 'weights.pt'\n",
        "torch.save(model.state_dict(), WEIGHT_PATH)\n",
        "print(f\"Model weights saved to {WEIGHT_PATH}\")"
      ],
      "metadata": {
        "id": "74hY1nKtt9hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context = torch.zeros(1, context_length, dtype=torch.long).to(device)  # Example initial context\n",
        "    logits = model(context)\n",
        "    print(logits)"
      ],
      "metadata": {
        "id": "Ad1K-fMhnekq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, new_chars: int, context, context_length: int, int_to_char: dict) -> str:\n",
        "    res = []\n",
        "    for i in range(new_chars):\n",
        "        if len(context.T) > context_length:\n",
        "            context = context[:, -context_length:]\n",
        "        prediction = model(context) # B, T, Vocab_Size\n",
        "        last_time_step = prediction[:, -1, :] # B, Vocab_Size\n",
        "        probabilities = nn.functional.softmax(last_time_step, dim = -1)\n",
        "        next_char = torch.multinomial(probabilities, 1)\n",
        "        context = torch.cat((context, next_char), dim = -1)\n",
        "        res.append(int_to_char[next_char.item()])\n",
        "    return ''.join(res)"
      ],
      "metadata": {
        "id": "7Ml8Ld3AkZVi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_chars = 1000 # Number of characters to generate\n",
        "context = torch.ones(1, 128, dtype = torch.int64).to(device)  # Initial context, e.g., all 1 tensor\n",
        "generated_text = generate(model, new_chars, context,context_length, dataset.idx_to_char)\n",
        "\n",
        "# Print or use the generated text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "F5fENK1GkcNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meaningful context\n",
        "\n",
        "initial_context = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife\"\n",
        "\n",
        "initial_context = initial_context[:context_length].ljust(context_length)\n",
        "\n",
        "# Encode the initial context\n",
        "encoded_context = [dataset.char_to_idx[ch] for ch in initial_context]\n",
        "context_tensor = torch.tensor(encoded_context, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Print the initial context and its tensor\n",
        "print(\"Initial context (string):\", initial_context)\n",
        "print(\"Initial context (tensor):\", context_tensor)\n",
        "\n",
        "generated_text =generate(model, new_chars=500, context=context_tensor, context_length=context_length, int_to_char=dataset.idx_to_char)\n",
        "\n",
        "print(\"Generated text:\", generated_text)\n"
      ],
      "metadata": {
        "id": "KUv6NEkvkTiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1Qsjumohv-G"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}