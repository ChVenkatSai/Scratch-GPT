{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChVenkatSai/Scratch-GPT/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtyping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxMhTqs9hJlU",
        "outputId": "7e80fe57-2987-4fb4-e499-1f2e75efd4e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.3.0+cu121)\n",
            "Collecting typeguard>=2.11.1 (from torchtyping)\n",
            "  Downloading typeguard-4.3.0-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.0->torchtyping)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->torchtyping)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Installing collected packages: typeguard, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtyping\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchtyping-0.1.4 typeguard-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iEUGLw7qcBkC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "from torchtyping import TensorType\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(0)\n",
        "        self.context_length = context_length\n",
        "        self.model_dim = model_dim\n",
        "        self.word = nn.Embedding(vocab_size,model_dim)\n",
        "        self.position = nn.Embedding(context_length,model_dim)\n",
        "        od = OrderedDict()\n",
        "        for i in range(num_blocks):\n",
        "            od.update({f\"block {i}\":self.TransformerBlock(model_dim,num_heads)})\n",
        "        self.layer = nn.Sequential(od)\n",
        "        self.norm = nn.LayerNorm(model_dim)\n",
        "        self.final = nn.Linear(model_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context: TensorType[int]) -> TensorType[float]:\n",
        "        torch.manual_seed(0)\n",
        "        word_embed = self.word(context)\n",
        "        batch_size = context.shape[0]\n",
        "        pos = torch.arange(self.context_length).to(device)\n",
        "        pos_embed = self.position(pos).unsqueeze(0).expand(batch_size,-1,-1)\n",
        "        embed = word_embed+pos_embed\n",
        "        block = self.layer(embed)\n",
        "        norm = self.norm(block)\n",
        "        fin = self.final(norm)\n",
        "        return fin\n",
        "\n",
        "    class TransformerBlock(nn.Module):\n",
        "\n",
        "      def __init__(self, model_dim: int, num_heads: int):\n",
        "          super().__init__()\n",
        "          torch.manual_seed(0)\n",
        "          self.first = nn.LayerNorm(model_dim)\n",
        "          self.attention = self.MultiHeadedSelfAttention(model_dim,model_dim,num_heads)\n",
        "          self.second = nn.LayerNorm(model_dim)\n",
        "          self.linear = self.VanillaNeuralNetwork(model_dim)\n",
        "\n",
        "      def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "          # Round answer to 4 decimal places\n",
        "          torch.manual_seed(0)\n",
        "          norm1 = self.first(embedded)\n",
        "          first = self.attention(norm1)\n",
        "          new = first+embedded\n",
        "          norm2 = self.second(new)\n",
        "          second = self.linear(norm2)\n",
        "          return second + new\n",
        "\n",
        "      class MultiHeadedSelfAttention(nn.Module):\n",
        "\n",
        "        def __init__(self, embedding_dim: int, attention_dim: int, num_heads: int):\n",
        "            super().__init__()\n",
        "            torch.manual_seed(0)\n",
        "            # Hint: nn.ModuleList() will be useful. It works the same as a Python list\n",
        "            # but is useful here since instance variables of any subclass of nn.Module\n",
        "            # must also be subclasses of nn.Module\n",
        "\n",
        "            # Use self.SingleHeadAttention(embedding_dim, head_size) to instantiate. You have to calculate head_size.\n",
        "            self.head_size = attention_dim//num_heads\n",
        "            self.multiattention = nn.ModuleList([self.SingleHeadAttention(embedding_dim,self.head_size) for _ in range(num_heads)])\n",
        "\n",
        "\n",
        "        def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "            # Return answer to 4 decimal places\n",
        "            l = []\n",
        "            for layer in (self.multiattention):\n",
        "                l.append(layer(embedded))\n",
        "            return torch.cat(l, dim=2)\n",
        "\n",
        "        class SingleHeadAttention(nn.Module):\n",
        "\n",
        "          def __init__(self, embedding_dim: int, attention_dim: int):\n",
        "              super().__init__()\n",
        "              torch.manual_seed(0)\n",
        "              self.attention_dim = attention_dim\n",
        "              self.key = nn.Linear(embedding_dim,attention_dim,bias=False)\n",
        "              self.query = nn.Linear(embedding_dim,attention_dim,bias=False)\n",
        "              self.value = nn.Linear(embedding_dim,attention_dim,bias=False)\n",
        "\n",
        "          def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "              # Return your answer to 4 decimal places\n",
        "              keys = self.key(embedded)\n",
        "              queries = self.query(embedded)\n",
        "              values = self.value(embedded)\n",
        "              attention = torch.matmul(queries,torch.transpose(keys,1,2))\n",
        "              attention = attention/torch.sqrt(torch.tensor(self.attention_dim))\n",
        "              mask = torch.ones(attention.shape)\n",
        "              mask = torch.triu(mask,diagonal=1).bool()\n",
        "              masked = attention.masked_fill(mask.to(device), float('-inf'))\n",
        "              norm = nn.functional.softmax(masked,dim=2)\n",
        "              final = torch.matmul(norm,values)\n",
        "              return final\n",
        "\n",
        "      class VanillaNeuralNetwork(nn.Module):\n",
        "\n",
        "        def __init__(self, model_dim: int):\n",
        "            super().__init__()\n",
        "            torch.manual_seed(0)\n",
        "            self.up_projection = nn.Linear(model_dim, model_dim * 4)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.down_projection = nn.Linear(model_dim * 4, model_dim)\n",
        "            self.dropout = nn.Dropout(0.2) # using p = 0.2\n",
        "\n",
        "        def forward(self, x: TensorType[float]) -> TensorType[float]:\n",
        "            torch.manual_seed(0)\n",
        "            return self.dropout(self.down_projection(self.relu(self.up_projection(x))))"
      ],
      "metadata": {
        "id": "zHAI6-2icJ00"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(self, model, new_chars: int, context: TensorType[int], context_length: int, int_to_char: dict) -> str:\n",
        "\n",
        "        generator = torch.manual_seed(0)\n",
        "        initial_state = generator.get_state()\n",
        "        res=\"\"\n",
        "        for i in range(new_chars):\n",
        "            if context.shape[1]>context_length:\n",
        "                output = model(context[:,-context_length:])\n",
        "            else:\n",
        "                output = model(context)\n",
        "            output = nn.functional.softmax(output,dim=2)\n",
        "            samples = torch.multinomial(output.squeeze(0),1,generator=generator)\n",
        "            # YOUR CODE (arbitrary number of lines)\n",
        "            # The line where you call torch.multinomial(). Pass in the generator as well.\n",
        "            generator.set_state(initial_state)\n",
        "            res+=int_to_char[int(samples.squeeze(0))]\n",
        "            context = torch.cat((context,samples),dim=-1)\n",
        "\n",
        "        return res"
      ],
      "metadata": {
        "id": "Ys0w5UJLdheS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "import random"
      ],
      "metadata": {
        "id": "G5JakALpqYh4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Download a book from Project Gutenberg\n",
        "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"  # Example: Pride and Prejudice\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower()\n",
        "text = text.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
        "text = ''.join([c for c in text if c.isalpha() or c.isspace()])\n",
        "\n",
        "# Prepare the dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_length):\n",
        "        self.text = text\n",
        "        self.context_length = context_length\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.vocab)}\n",
        "        self.data = self.encode(text)\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data[idx:idx + self.context_length]\n",
        "        target = self.data[idx + 1:idx + self.context_length + 1]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(set(text))\n",
        "context_length = 128\n",
        "model_dim = 512\n",
        "num_blocks = 6\n",
        "num_heads = 8\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Instantiate the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = TextDataset(text, context_length)\n",
        "subset_indices = random.sample(range(len(dataset)), k=100000)  # Choose 5000 samples randomly\n",
        "print(\"Unencoded text:\", dataset.text[:context_length + 1])\n",
        "print(dataset[0])\n",
        "subset_dataset = Subset(dataset, subset_indices)\n",
        "dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (context, target) in enumerate(dataloader):\n",
        "        context, target = context.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(context)\n",
        "\n",
        "        # Reshape output and target to match the dimensions for CrossEntropyLoss\n",
        "        output = output.view(-1, vocab_size)\n",
        "        target = target.view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nYpNdzyanRAC",
        "outputId": "d00e26c9-413c-405d-e15a-373e9c2158f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unencoded text: ïthe project gutenberg ebook of pride and prejudice by jane austen    this ebook is for the use of anyone anywhere in the united \n",
            "(tensor([32, 20,  8,  5,  0, 16, 18, 15, 10,  5,  3, 20,  0,  7, 21, 20,  5, 14,\n",
            "         2,  5, 18,  7,  0,  5,  2, 15, 15, 11,  0, 15,  6,  0, 16, 18,  9,  4,\n",
            "         5,  0,  1, 14,  4,  0, 16, 18,  5, 10, 21,  4,  9,  3,  5,  0,  2, 25,\n",
            "         0, 10,  1, 14,  5,  0,  1, 21, 19, 20,  5, 14,  0,  0,  0,  0, 20,  8,\n",
            "         9, 19,  0,  5,  2, 15, 15, 11,  0,  9, 19,  0,  6, 15, 18,  0, 20,  8,\n",
            "         5,  0, 21, 19,  5,  0, 15,  6,  0,  1, 14, 25, 15, 14,  5,  0,  1, 14,\n",
            "        25, 23,  8,  5, 18,  5,  0,  9, 14,  0, 20,  8,  5,  0, 21, 14,  9, 20,\n",
            "         5,  4]), tensor([20,  8,  5,  0, 16, 18, 15, 10,  5,  3, 20,  0,  7, 21, 20,  5, 14,  2,\n",
            "         5, 18,  7,  0,  5,  2, 15, 15, 11,  0, 15,  6,  0, 16, 18,  9,  4,  5,\n",
            "         0,  1, 14,  4,  0, 16, 18,  5, 10, 21,  4,  9,  3,  5,  0,  2, 25,  0,\n",
            "        10,  1, 14,  5,  0,  1, 21, 19, 20,  5, 14,  0,  0,  0,  0, 20,  8,  9,\n",
            "        19,  0,  5,  2, 15, 15, 11,  0,  9, 19,  0,  6, 15, 18,  0, 20,  8,  5,\n",
            "         0, 21, 19,  5,  0, 15,  6,  0,  1, 14, 25, 15, 14,  5,  0,  1, 14, 25,\n",
            "        23,  8,  5, 18,  5,  0,  9, 14,  0, 20,  8,  5,  0, 21, 14,  9, 20,  5,\n",
            "         4,  0]))\n",
            "Epoch [1/5], Step [0/1563], Loss: 3.7389\n",
            "Epoch [1/5], Step [100/1563], Loss: 2.3135\n",
            "Epoch [1/5], Step [200/1563], Loss: 1.7998\n",
            "Epoch [1/5], Step [300/1563], Loss: 1.4074\n",
            "Epoch [1/5], Step [400/1563], Loss: 1.2646\n",
            "Epoch [1/5], Step [500/1563], Loss: 1.1956\n",
            "Epoch [1/5], Step [600/1563], Loss: 1.1153\n",
            "Epoch [1/5], Step [700/1563], Loss: 1.1364\n",
            "Epoch [1/5], Step [800/1563], Loss: 1.0618\n",
            "Epoch [1/5], Step [900/1563], Loss: 1.0478\n",
            "Epoch [1/5], Step [1000/1563], Loss: 1.0241\n",
            "Epoch [1/5], Step [1100/1563], Loss: 0.9978\n",
            "Epoch [1/5], Step [1200/1563], Loss: 0.9943\n",
            "Epoch [1/5], Step [1300/1563], Loss: 0.9588\n",
            "Epoch [1/5], Step [1400/1563], Loss: 0.8935\n",
            "Epoch [1/5], Step [1500/1563], Loss: 0.8935\n",
            "Epoch [2/5], Step [0/1563], Loss: 0.8333\n",
            "Epoch [2/5], Step [100/1563], Loss: 0.8481\n",
            "Epoch [2/5], Step [200/1563], Loss: 0.8102\n",
            "Epoch [2/5], Step [300/1563], Loss: 0.7789\n",
            "Epoch [2/5], Step [400/1563], Loss: 0.7922\n",
            "Epoch [2/5], Step [500/1563], Loss: 0.7298\n",
            "Epoch [2/5], Step [600/1563], Loss: 0.7209\n",
            "Epoch [2/5], Step [700/1563], Loss: 0.6893\n",
            "Epoch [2/5], Step [800/1563], Loss: 0.6510\n",
            "Epoch [2/5], Step [900/1563], Loss: 0.6011\n",
            "Epoch [2/5], Step [1000/1563], Loss: 0.6233\n",
            "Epoch [2/5], Step [1100/1563], Loss: 0.6164\n",
            "Epoch [2/5], Step [1200/1563], Loss: 0.5713\n",
            "Epoch [2/5], Step [1300/1563], Loss: 0.5150\n",
            "Epoch [2/5], Step [1400/1563], Loss: 0.5146\n",
            "Epoch [2/5], Step [1500/1563], Loss: 0.5000\n",
            "Epoch [3/5], Step [0/1563], Loss: 0.4448\n",
            "Epoch [3/5], Step [100/1563], Loss: 0.4329\n",
            "Epoch [3/5], Step [200/1563], Loss: 0.4124\n",
            "Epoch [3/5], Step [300/1563], Loss: 0.4076\n",
            "Epoch [3/5], Step [400/1563], Loss: 0.3899\n",
            "Epoch [3/5], Step [500/1563], Loss: 0.3715\n",
            "Epoch [3/5], Step [600/1563], Loss: 0.3572\n",
            "Epoch [3/5], Step [700/1563], Loss: 0.3243\n",
            "Epoch [3/5], Step [800/1563], Loss: 0.3171\n",
            "Epoch [3/5], Step [900/1563], Loss: 0.2880\n",
            "Epoch [3/5], Step [1000/1563], Loss: 0.3048\n",
            "Epoch [3/5], Step [1100/1563], Loss: 0.2989\n",
            "Epoch [3/5], Step [1200/1563], Loss: 0.2885\n",
            "Epoch [3/5], Step [1300/1563], Loss: 0.2800\n",
            "Epoch [3/5], Step [1400/1563], Loss: 0.2740\n",
            "Epoch [3/5], Step [1500/1563], Loss: 0.2643\n",
            "Epoch [4/5], Step [0/1563], Loss: 0.2500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-80b8cfe19597>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training is complete\n",
        "WEIGHT_PATH = 'weights.pt'\n",
        "torch.save(model.state_dict(), WEIGHT_PATH)\n",
        "print(f\"Model weights saved to {WEIGHT_PATH}\")"
      ],
      "metadata": {
        "id": "74hY1nKtt9hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f22c088-e841-4ebd-863d-47972a184ea4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved to weights.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context = torch.zeros(1, context_length, dtype=torch.long).to(device)  # Example initial context\n",
        "    logits = model(context)\n",
        "    print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad1K-fMhnekq",
        "outputId": "1b2718ae-7f99-4539-a15f-6725be723637"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[  1.5186,   1.3980,   0.0777,  ...,  -7.7479,  -8.6362,  -9.4504],\n",
            "         [  3.1690,   0.5301,  -0.1313,  ...,  -7.7939,  -8.2301,  -9.4291],\n",
            "         [  4.7322,   0.3038,   0.2443,  ...,  -5.9061,  -7.0487,  -8.8097],\n",
            "         ...,\n",
            "         [  7.0719,  -0.2045,  -0.5549,  ...,  -8.0985,  -7.6886, -10.0902],\n",
            "         [  7.5740,   0.1351,  -0.0592,  ...,  -8.2591,  -7.7744, -10.7178],\n",
            "         [  7.9012,   0.0654,  -0.1203,  ...,  -8.0369,  -7.8386, -10.6654]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, new_chars: int, context, context_length: int, int_to_char: dict) -> str:\n",
        "    res = []\n",
        "    for i in range(new_chars):\n",
        "        if len(context.T) > context_length:\n",
        "            context = context[:, -context_length:]\n",
        "        prediction = model(context) # B, T, Vocab_Size\n",
        "        last_time_step = prediction[:, -1, :] # B, Vocab_Size\n",
        "        probabilities = nn.functional.softmax(last_time_step, dim = -1)\n",
        "        next_char = torch.multinomial(probabilities, 1)\n",
        "        context = torch.cat((context, next_char), dim = -1)\n",
        "        res.append(int_to_char[next_char.item()])\n",
        "    return ''.join(res)"
      ],
      "metadata": {
        "id": "7Ml8Ld3AkZVi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a trained model and the TextDataset class from earlier\n",
        "\n",
        "# Define the context length\n",
        "context_length = 128\n",
        "\n",
        "# Your initial context as a string\n",
        "initial_context = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife\"\n",
        "\n",
        "# Ensure the initial context is exactly `context_length` characters long\n",
        "# Truncate if it's longer, or pad with spaces if it's shorter\n",
        "initial_context = initial_context[:context_length].ljust(context_length)\n",
        "\n",
        "# Encode the initial context\n",
        "encoded_context = [dataset.char_to_idx[ch] for ch in initial_context]\n",
        "\n",
        "# Convert the list of indices to a PyTorch tensor\n",
        "context_tensor = torch.tensor(encoded_context, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Print the initial context and its tensor\n",
        "print(\"Initial context (string):\", initial_context)\n",
        "print(\"Initial context (tensor):\", context_tensor)\n",
        "\n",
        "# Generate text using the model\n",
        "with torch.no_grad():\n",
        "    generated_text =generate(model, new_chars=500, context=context_tensor, context_length=context_length, int_to_char=dataset.idx_to_char)\n",
        "\n",
        "print(\"Generated text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUv6NEkvkTiD",
        "outputId": "06393983-1cf2-49e2-f771-f79eb9dc9d63"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial context (string): it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife              \n",
            "Initial context (tensor): tensor([[ 9, 20,  0,  9, 19,  0,  1,  0, 20, 18, 21, 20,  8,  0, 21, 14,  9, 22,\n",
            "          5, 18, 19,  1, 12, 12, 25,  0,  1,  3, 11, 14, 15, 23, 12,  5,  4,  7,\n",
            "          5,  4,  0, 20,  8,  1, 20,  0,  1,  0, 19,  9, 14,  7, 12,  5,  0, 13,\n",
            "          1, 14,  0,  9, 14,  0, 16, 15, 19, 19,  5, 19, 19,  9, 15, 14,  0, 15,\n",
            "          6,  0,  1,  0,  7, 15, 15,  4,  0,  6, 15, 18, 20, 21, 14,  5,  0, 13,\n",
            "         21, 19, 20,  0,  2,  5,  0,  9, 14,  0, 23,  1, 14, 20,  0, 15,  6,  0,\n",
            "          1,  0, 23,  9,  6,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0]], device='cuda:0')\n",
            "Generated text:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_chars = 1000 # Number of characters to generate\n",
        "context = torch.ones(1, 128, dtype = torch.int64).to(device)  # Initial context, e.g., empty tensor\n",
        "generated_text = generate(model, new_chars, context,context_length, dataset.idx_to_char)\n",
        "\n",
        "# Print or use the generated text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5fENK1GkcNg",
        "outputId": "dd5d323d-3c39-45ff-aa6a-0dff01fc5ef8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n  heart of what i said of the very begin to         with a proper a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1Qsjumohv-G"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}